<analysis>
The trajectory documents a multi-stage debugging and refactoring process of an AI-powered plan generation application. The initial task was to resolve a  by migrating to an asynchronous job-based architecture, which was successfully implemented using a separate  process managed by .

The subsequent work focused on making the system production-ready, as directed by the user. This involved a series of iterative refinements:
1.  **API Key Unification:** The system initially used two different keys ( and ). At the user's request, the entire codebase was refactored to exclusively use a single , removing all dependencies on the  library for LLM calls.
2.  **Implementation of Controls:** The user requested critical controls for cost and stability. This involved adding  to all OpenAI calls, reducing the default , implementing token usage tracking (by capturing  and storing it in MongoDB), and adding a user-level daily rate limit.
3.  **Validation and Debugging:** The validation phase revealed deeper issues. Jobs failed repeatedly due to  errors and JSON parsing failures in agent . The root cause was diagnosed as a snowballing context problem, where each agent passes an increasingly large data object to the next, causing later agents to exceed token limits and fail.

The conversation concluded with the user requesting a major architectural refactor of the inter-agent data flow to solve this core issue. The AI has confirmed its understanding and is awaiting the go-ahead to begin this new, more structural task.

The user's primary language is Spanish. The next agent must respond in **Spanish**.
</analysis>

<product_requirements>
The primary goal is to build a stable, robust, and cost-controlled AI plan generation system.

**Initial Requirement (Completed):**
Resolve  errors by converting the long-running plan generation into an asynchronous job-based system. This was achieved by creating a  endpoint that queues a job in MongoDB, which is then processed by a separate background worker.

**Evolved Requirement (In Progress):**
Refactor the internal data flow between AI agents (E1-E9, N0-N8) to prevent the snowballing context problem. Instead of each agent inheriting the entire accumulated data from previous agents, each agent should only receive a compact base profile plus the specific, minimal outputs (cajones or drawers) it needs from its predecessors.

**Objectives of the Refactor:**
1.  **Reduce Token Consumption:** Drastically lower the input tokens for later-stage agents to reduce operational costs.
2.  **Prevent Failures:** Eliminate errors caused by exceeding context or output token limits (e.g., the E5 agent JSON truncation).
3.  **Improve Maintainability:** Create a more modular and predictable data flow.
</product_requirements>

<key_technical_concepts>
- **Asynchronous Job Queue:** Using MongoDB as a message broker for long-running tasks.
- **Decoupled Worker Process:** A persistent Python script () managed by  to process jobs independently of the API server.
- **API Key Management:** Centralizing all LLM calls to use a single  environment variable.
- **Cost & Stability Controls:** Implementing rate limiting, request timeouts,  limits, and token usage tracking.
- **Context Scoping:** The new architectural concept of passing minimal, relevant data between chained AI agents instead of a continuously growing context object.
</key_technical_concepts>

<code_architecture>
The architecture consists of a FastAPI web server for handling API requests and a separate Python background worker for processing long-running AI jobs. MongoDB serves as the database and job queue.

**Directory Structure:**


- ****
    - **Importance:** The parent class for all primary AI agents (E1-E9, N0-N8). Centralizes LLM call logic.
    - **Changes:**
        - Refactored to use a single .
        - Added  to all OpenAI calls.
        -  was reduced to , with a dynamic override allowing specific agents like  to use a higher limit ().
        - Implemented logic to capture  to track token consumption.
        - A robust JSON parsing method () with retry and self-correction capabilities was added to handle malformed LLM outputs.

- ****
    - **Importance:** The main FastAPI application. Defines API endpoints, including the job creation and status polling routes. It also contains the core job processing logic called by the worker.
    - **Changes:**
        - Rate limiting logic was added to the  endpoint.
        - All  LLM calls were replaced with the standard usage: openai [-h] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]]
              [-o ORGANIZATION] [-t {openai,azure}]
              [--api-version API_VERSION] [--azure-endpoint AZURE_ENDPOINT]
              [--azure-ad-token AZURE_AD_TOKEN] [-V]
              {api,tools,migrate,grit} ...

positional arguments:
  {api,tools,migrate,grit}
    api                 Direct API calls
    tools               Client side tools for convenience

options:
  -h, --help            show this help message and exit
  -v, --verbose         Set verbosity.
  -b API_BASE, --api-base API_BASE
                        What API base url to use.
  -k API_KEY, --api-key API_KEY
                        What API key to use.
  -p PROXY [PROXY ...], --proxy PROXY [PROXY ...]
                        What proxy to use.
  -o ORGANIZATION, --organization ORGANIZATION
                        Which organization to run as (will use your default
                        organization if not specified)
  -t {openai,azure}, --api-type {openai,azure}
                        The backend API to call, must be `openai` or `azure`
  --api-version API_VERSION
                        The Azure API version, e.g.
                        'https://learn.microsoft.com/en-us/azure/ai-
                        services/openai/reference#rest-api-versioning'
  --azure-endpoint AZURE_ENDPOINT
                        The Azure endpoint, e.g.
                        'https://endpoint.openai.azure.com'
  --azure-ad-token AZURE_AD_TOKEN
                        A token from Azure Active Directory,
                        https://www.microsoft.com/en-
                        us/security/business/identity-access/microsoft-entra-
                        id
  -V, --version         show program's version number and exit SDK.
        - The job processing logic was updated to accumulate token usage data from each agent and save it to the MongoDB job document.
        - A final log message summarizing total token usage per job was added upon completion.

- ****
    - **Importance:** Defines all Pydantic data models.
    - **Changes:**
        - New models  and  were added to structure the token consumption data.
        - The main  model was updated to include the new  field, enabling detailed cost tracking.

- ****
    - **Importance:** Handles the Free Initial Diagnosis feature.
    - **Changes:**
        - Completely refactored to use the standard usage: openai [-h] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]]
              [-o ORGANIZATION] [-t {openai,azure}]
              [--api-version API_VERSION] [--azure-endpoint AZURE_ENDPOINT]
              [--azure-ad-token AZURE_AD_TOKEN] [-V]
              {api,tools,migrate,grit} ...

positional arguments:
  {api,tools,migrate,grit}
    api                 Direct API calls
    tools               Client side tools for convenience

options:
  -h, --help            show this help message and exit
  -v, --verbose         Set verbosity.
  -b API_BASE, --api-base API_BASE
                        What API base url to use.
  -k API_KEY, --api-key API_KEY
                        What API key to use.
  -p PROXY [PROXY ...], --proxy PROXY [PROXY ...]
                        What proxy to use.
  -o ORGANIZATION, --organization ORGANIZATION
                        Which organization to run as (will use your default
                        organization if not specified)
  -t {openai,azure}, --api-type {openai,azure}
                        The backend API to call, must be `openai` or `azure`
  --api-version API_VERSION
                        The Azure API version, e.g.
                        'https://learn.microsoft.com/en-us/azure/ai-
                        services/openai/reference#rest-api-versioning'
  --azure-endpoint AZURE_ENDPOINT
                        The Azure endpoint, e.g.
                        'https://endpoint.openai.azure.com'
  --azure-ad-token AZURE_AD_TOKEN
                        A token from Azure Active Directory,
                        https://www.microsoft.com/en-
                        us/security/business/identity-access/microsoft-entra-
                        id
  -V, --version         show program's version number and exit library and , removing its previous dependency on  and .

- ****
    - **Importance:** The standalone worker process that polls MongoDB for pending jobs and executes them. It ensures the API remains responsive.
    - **Changes:** No major recent changes. Its stability was key to the architecture. It contains the main loop and a timeout watchdog.

- ****
    - **Importance:** Stores environment variables.
    - **Changes:** The user is now expected to provide only one key, , for all LLM functionalities. The  is no longer used for LLM calls.
</code_architecture>

<pending_tasks>
- **Primary Task:** Refactor the inter-agent data flow to implement a scoped context or cajones model, preventing the snowballing context issue.
- **On Hold:** The full validation and stress testing (3 consecutive jobs, 2 concurrent jobs, 1 forced failure) is pending the completion of the data flow refactor.
</pending_tasks>

<current_work>
The immediate work is to refactor the data flow between agents as per the user's detailed conceptual request. The previous effort to validate the system failed due to a structural flaw: each agent in the chain (E1->E2->...E9) passes its entire accumulated context to the next, causing the input for later agents like  to become enormous. This leads to failures from exceeding token limits () and exacerbates issues with OpenAI rate limiting.

The user has explicitly approved a plan to change this. The new approach involves passing a small, constant  to every agent, plus only the specific, minimal outputs from direct predecessor agents that are necessary for the current agent's task.

The last action was the AI confirming its complete understanding of this refactoring goal, analyzing the risks and benefits, and stating its readiness to begin the implementation, starting with  (to define the new data cajones) and  (to build the minimal context for each agent). No code for this refactor has been written yet.
</current_work>

<optional_next_step>
Begin the data flow refactoring by modifying  to define the new cajones structure and then updating  to build and pass the minimal context to each agent.
</optional_next_step>
